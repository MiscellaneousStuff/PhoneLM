{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhoneLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `G2P` and `Encodec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install g2p_en encodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `G2P`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import string\n",
    "from functools import cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "@cache\n",
    "def _get_model():\n",
    "    return G2p()\n",
    "\n",
    "@cache\n",
    "def _get_graphs(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        graphs = f.read()\n",
    "    return graphs\n",
    "\n",
    "def encode(graphs: str) -> list[str]:\n",
    "    g2p = _get_model()\n",
    "    phones = g2p(graphs)\n",
    "    ignored = {\" \", *string.punctuation}\n",
    "    return [\"_\" if p in ignored else p for p in phones]\n",
    "\n",
    "@torch.no_grad()\n",
    "def write_phones(folder, suffix=\".normalized.txt\"):\n",
    "    print(\"ello?\")\n",
    "    paths = list(folder.rglob(f\"*{suffix}\"))\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    print(\"paths:\", paths)\n",
    "    for path in tqdm(paths):\n",
    "        phone_path = path.with_name(path.stem.split(\".\")[0] + \".phn.txt\")\n",
    "        if phone_path.exists():\n",
    "            continue\n",
    "        print(\"?\")\n",
    "        graphs = _get_graphs(path)\n",
    "        phones = encode(graphs)\n",
    "        with open(phone_path, \"w\") as f:\n",
    "            f.write(\" \".join(phones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ello?\n",
      "paths: [WindowsPath('data/text/test.normalized.txt')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "write_phones(Path(\"./data/text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Encodec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from functools import cache\n",
    "import torchaudio\n",
    "from encodec import EncodecModel\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "import soundfile\n",
    "from encodec.utils import convert_audio\n",
    "\n",
    "SAMPLE_RATE = 24_000\n",
    "BANDWIDTHS  = [1.5, 3.0, 6.0, 12.0, 24.0]\n",
    "BANDWIDTH   = BANDWIDTHS[0]\n",
    "\n",
    "@cache\n",
    "def _load_model(bandwidth=6.0, device=\"cuda\"):\n",
    "    # Instantiate a pretrained EnCodec model\n",
    "    assert SAMPLE_RATE == 24_000\n",
    "    model = EncodecModel.encodec_model_24khz()\n",
    "    model.set_target_bandwidth(bandwidth)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def unload_model():\n",
    "    return _load_model.cache_clear()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def decode(codes: Tensor, bandwidth=6.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        codes: (b q t)\n",
    "    \"\"\"\n",
    "    assert codes.dim() == 3\n",
    "    model = _load_model(bandwidth, device)\n",
    "    return model.decode([(codes, None)]), model.sample_rate\n",
    "\n",
    "def decode_to_file(resps: Tensor, path: Path):\n",
    "    assert resps.dim() == 2, f\"Require shape (t q), but got {resps.shape}.\"\n",
    "    resps = rearrange(resps, \"t q -> 1 q t\")\n",
    "    wavs, sr = decode(codes=resps, bandwidth=BANDWIDTH)\n",
    "    soundfile.write(str(path), wavs.cpu()[0, 0], sr)\n",
    "\n",
    "def _replace_file_extension(path, suffix):\n",
    "    return (path.parent / path.name.split(\".\")[0]).with_suffix(suffix)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode(wav: Tensor, sr: int, bandwidth=6.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        wav: (t)\n",
    "        sr: int\n",
    "    \"\"\"\n",
    "    model = _load_model(bandwidth, device)\n",
    "    wav = wav.unsqueeze(0)\n",
    "    wav = convert_audio(wav, sr, model.sample_rate, model.channels)\n",
    "    wav = wav.to(device)\n",
    "    encoded_frames = model.encode(wav)\n",
    "    qnt = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # (b q t)\n",
    "    return qnt\n",
    "\n",
    "def encode_from_file(path, bandwidth=6.0, device=\"cuda\"):\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    if wav.shape[0] == 2:\n",
    "        wav = wav[:1]\n",
    "    return encode(wav, sr, bandwidth, device)\n",
    "\n",
    "def quantize_audio(folder, suffix=\".wav\"):\n",
    "    paths = [*folder.rglob(f\"*{suffix}\")]\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        out_path = _replace_file_extension(path, \".qnt.pt\")\n",
    "        if out_path.exists():\n",
    "            continue\n",
    "        qnt = encode_from_file(path, BANDWIDTH)\n",
    "        print(qnt.shape)\n",
    "        torch.save(qnt.cpu(), out_path)\n",
    "\n",
    "def decode_files(folder, suffix=\".qnt.pt\"):\n",
    "    paths = [*folder.rglob(f\"*{suffix}\")]\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        out_path = _replace_file_extension(path, \".qt.wav\")\n",
    "        if out_path.exists():\n",
    "            continue\n",
    "        fi = rearrange(torch.load(path).squeeze(0).cuda(), \"q t -> t q\")\n",
    "        decode_to_file(fi, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "quantize_audio(Path(\"./data/audio\"))\n",
    "decode_files(Path(\"./data/audio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"data/audio/test.qnt.pt\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LJSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDWIDTH_IDX = 1 # original VALL-E\n",
    "CODEBOOKS     = [2, 4, 8, 16, 32]\n",
    "BANDWIDTHS    = [1.5, 3.0, 6.0, 12.0, 24.0]\n",
    "BANDWIDTH     = BANDWIDTHS[BANDWIDTH_IDX]\n",
    "CODEBOOK      = CODEBOOKS[BANDWIDTH_IDX]\n",
    "\n",
    "import torchaudio\n",
    "from ljspeech import LJSPEECH\n",
    "DATASET_PATH = \"./data/LJSpeech/\"\n",
    "dataset = LJSPEECH(\n",
    "    \"./data/LJSpeech\",\n",
    "    encodec_bandwidth=BANDWIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13100"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 725])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler, collate_fn=lambda x: x)\n",
    "test_loader = DataLoader(dataset, batch_size=32, sampler=test_sampler, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 41)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import megabyte\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "def get_reserved_mem_gb():\n",
    "    device = torch.cuda.current_device()\n",
    "    reserved = torch.cuda.memory_reserved(device)\n",
    "    reserved_gb = reserved / 1024 / 1024 / 1024\n",
    "    return reserved_gb\n",
    "\n",
    "class PhoneLM(nn.Module):\n",
    "    def __init__(self, n_phone_tokens, n_audio_tokens):\n",
    "        super(PhoneLM, self).__init__()\n",
    "        self.megabyte   = megabyte.MEGABYTE(\n",
    "            heads       = 8, # 1,\n",
    "            dim_head    = 32, # 16,\n",
    "            num_tokens  = n_phone_tokens + n_audio_tokens + 4,\n",
    "            dim         = (768, 256, 128), # (32, 32, 32), # (768, 256, 128)# Dg, Dl1, Dl2\n",
    "            depth       = (6, 4, 2), # (6, 4, 2)\n",
    "            max_seq_len = (128, 4, 4), # (32, 4, 4), # 512\n",
    "            flash_attn  = False)\n",
    "\n",
    "    def forward(self, x, debug=False, return_loss=True):\n",
    "        x = self.megabyte(x, return_loss=return_loss)\n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        o = [param.numel() for param in self.parameters() if param.requires_grad]\n",
    "        o = sum(o)\n",
    "        return o\n",
    "    \n",
    "    def generate(self, *args):\n",
    "        return self.megabyte.generate(*args)\n",
    "    \n",
    "def multi_encode(\n",
    "        phone_tokens,\n",
    "        audio_tokens,\n",
    "        n_phone_tokens,\n",
    "        n_audio_tokens,\n",
    "        max_clip_length=1.0):\n",
    "    \"\"\"NOTE: 75 steps per second for 24kHz in `encodec.\n",
    "    Set `max_clip_length` to 0 for original clip length.\"\"\"\n",
    "\n",
    "    # Start text token, end text token, start audio token, end audio token\n",
    "    STT, ETT, SAT, EAT = [n_phone_tokens + n_audio_tokens + i\n",
    "                          for i in range(4)]\n",
    "    STT = torch.tensor([STT]).long().cuda()\n",
    "    ETT = torch.tensor([ETT]).long().cuda()\n",
    "    SAT = torch.tensor([SAT]).long().cuda()\n",
    "    EAT = torch.tensor([EAT]).long().cuda()\n",
    "\n",
    "    if max_clip_length:\n",
    "        print(\"pre audio_tokens.shape\", audio_tokens.shape)\n",
    "        audio_tokens = audio_tokens[:, :, :int(max_clip_length * 75)]\n",
    "    audio_tokens = rearrange(audio_tokens.squeeze(0), \"q s -> (q s)\")\n",
    "    print(\"post audio_tokens.shape\", audio_tokens.shape)\n",
    "    \n",
    "    # offset phone tokens past audio tokens\n",
    "    phone_tokens += n_audio_tokens\n",
    "    \n",
    "    print(\"phone_tokens.shape:\", phone_tokens.shape)\n",
    "    print(\"audio_tokens.shape:\", audio_tokens.shape)\n",
    "    \n",
    "    device = torch.cuda.current_device()\n",
    "    phone_tokens = torch.cat((STT, phone_tokens, ETT), dim=0).to(device)\n",
    "    audio_tokens = torch.cat((SAT, audio_tokens, EAT,), dim=0).to(device)\n",
    "    combined_tokens = torch.cat((phone_tokens, audio_tokens), dim=0).to(device)\n",
    "    return phone_tokens, audio_tokens, combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "from encodec_util import decode_to_file\n",
    "\n",
    "def generate_audio(sample,\n",
    "                   n_phone_tokens,\n",
    "                   n_audio_tokens,\n",
    "                   audio_path=\"./out.wav\"):\n",
    "    STT, ETT, SAT, EAT = [n_phone_tokens + n_audio_tokens + i\n",
    "                          for i in range(4)]\n",
    "    ST_S = [STT, ETT, SAT, EAT]\n",
    "    print(\"STT, ETT, SAT, EAT ids:\", ST_S)\n",
    "    seq = sample.cpu().tolist()[0]\n",
    "    print(\"seq:\", seq)\n",
    "    # all special tokens in list\n",
    "    if all(st_t in seq for st_t in ST_S) and len(seq) >= len(ST_S) + 2:\n",
    "        # text_tokens  = seq[seq.index(STT + 1):seq.index(ETT - 1)]\n",
    "        audio_tokens = seq[seq.index(SAT)+1:seq.index(EAT)]\n",
    "        print(seq.index(SAT), seq.index(EAT), len(audio_tokens))\n",
    "        audio_tokens = torch.tensor(audio_tokens).cuda()\n",
    "        audio_tokens = rearrange(\n",
    "            audio_tokens,\n",
    "            '(t q) -> t q',\n",
    "            q=CODEBOOK,\n",
    "            t=audio_tokens.size(0) // CODEBOOK)\n",
    "        print(\"audio_tokens.shape:\", audio_tokens, audio_tokens.shape)\n",
    "        decode_to_file(audio_tokens, audio_path)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhoneLM - LJSpeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 37.30M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37302863"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PhoneLM(\n",
    "    n_phone_tokens=len(dataset.phone_dict),\n",
    "    n_audio_tokens=1024).to(device)\n",
    "\n",
    "model.megabyte.get_num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([88]), torch.Size([1, 4, 484]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(train_loader))[0]\n",
    "item_phone_tokens = item[-2]\n",
    "item_audio_tokens = item[-1]\n",
    "item_phone_tokens.shape, item_audio_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To specify more particularly one or two of the worst, it may be mentioned that in the Borough Compter'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre audio_tokens.shape torch.Size([1, 4, 484])\n",
      "post audio_tokens.shape torch.Size([1500])\n",
      "phone_tokens.shape: torch.Size([88])\n",
      "audio_tokens.shape: torch.Size([1500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1592])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_prompt, audio_target, test_inp = multi_encode(\n",
    "    item_phone_tokens,\n",
    "    item_audio_tokens,\n",
    "    n_phone_tokens=len(dataset.phone_dict),\n",
    "    n_audio_tokens=1024,\n",
    "    max_clip_length=5)\n",
    "test_inp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "MAX_LR       = 1e-2\n",
    "# MAX_LR       = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GRAD_CLIP    = 0.1\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=MAX_LR)\n",
    "    #,weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# def get_lr(optimizer):\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         return param_group['lr']\n",
    "\n",
    "# sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, MAX_LR, epochs=epochs, \n",
    "#                                                 steps_per_epoch=len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1099, 1084, 1091,  ...,  859,  222, 1102], device='cuda:0')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1592])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved Memory (GB): 3.08984375, loss: 7.007445335388184\n",
      "Reserved Memory (GB): 3.08984375, loss: 0.03312724456191063\n",
      "Reserved Memory (GB): 3.08984375, loss: 0.016318222507834435\n",
      "Reserved Memory (GB): 3.08984375, loss: 2.5148987770080566\n",
      "Reserved Memory (GB): 3.08984375, loss: 0.025118699297308922\n",
      "Reserved Memory (GB): 3.08984375, loss: 0.009873692877590656\n",
      "Reserved Memory (GB): 3.08984375, loss: 0.007557244971394539\n",
      "Reserved Memory (GB): 3.08984375, loss: 0.007007556967437267\n",
      "Reserved Memory (GB): 3.08984375, loss: 0.006757958326488733\n",
      "Reserved Memory (GB): 3.08984375, loss: 0.006608926225453615\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "EPOCHS = 1000\n",
    "PRINT_INTERVAL = 100\n",
    "\n",
    "seq_len = 2048\n",
    "\n",
    "def train(model, trainloader):\n",
    "    model.train()\n",
    "    \n",
    "    padding_len = max(0, seq_len - test_inp.size(0))\n",
    "    n_test_inp = F.pad(test_inp, (0, padding_len))\n",
    "    batch = n_test_inp.unsqueeze(0)\n",
    "    # print(batch.shape)\n",
    "    loss = model(batch, return_loss=True)\n",
    "    # loss = model(next(trainloader), return_loss=True)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# pbar = tqdm.tqdm(EPOCHS, mininterval=10., desc='training')\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(model, train_loader)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    mem_gb = get_reserved_mem_gb()\n",
    "    if epoch % PRINT_INTERVAL == 0:\n",
    "        print(f\"Reserved Memory (GB): {mem_gb}, loss: {loss.item()}\")\n",
    "    #' pbar.set_description(f\"Reserved Memory (GB): {mem_gb}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1101,   25,  574,  ...,  859,  222, 1102], device='cuda:0')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1958/1958 [00:26<00:00, 74.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: tensor([[1099, 1084, 1091,  ...,   43,  881,  784]], device='cuda:0') torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate(model, prompt):\n",
    "    model.eval()\n",
    "\n",
    "    prompt = prompt.unsqueeze(0)\n",
    "    sample = model.generate(prompt)\n",
    "    sample = sample.flatten(1)\n",
    "    print(\"sample:\", sample, sample.shape)\n",
    "\n",
    "    return prompt, sample\n",
    "\n",
    "prompt, sample = generate(model, phone_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14732/1048868286.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "STT, ETT, SAT, EAT = [len(dataset.phone_dict) + 1024 + i\n",
    "                          for i in range(4)]\n",
    "sample.index(STT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_audio(\n",
    "    sample,\n",
    "    n_phone_tokens=len(dataset.phone_dict),\n",
    "    n_audio_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
