{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhoneLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `G2P` and `Encodec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install g2p_en encodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `G2P`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import string\n",
    "from functools import cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "@cache\n",
    "def _get_model():\n",
    "    return G2p()\n",
    "\n",
    "@cache\n",
    "def _get_graphs(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        graphs = f.read()\n",
    "    return graphs\n",
    "\n",
    "def encode(graphs: str) -> list[str]:\n",
    "    g2p = _get_model()\n",
    "    phones = g2p(graphs)\n",
    "    ignored = {\" \", *string.punctuation}\n",
    "    return [\"_\" if p in ignored else p for p in phones]\n",
    "\n",
    "@torch.no_grad()\n",
    "def write_phones(folder, suffix=\".normalized.txt\"):\n",
    "    print(\"ello?\")\n",
    "    paths = list(folder.rglob(f\"*{suffix}\"))\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    print(\"paths:\", paths)\n",
    "    for path in tqdm(paths):\n",
    "        phone_path = path.with_name(path.stem.split(\".\")[0] + \".phn.txt\")\n",
    "        if phone_path.exists():\n",
    "            continue\n",
    "        print(\"?\")\n",
    "        graphs = _get_graphs(path)\n",
    "        phones = encode(graphs)\n",
    "        with open(phone_path, \"w\") as f:\n",
    "            f.write(\" \".join(phones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ello?\n",
      "paths: [WindowsPath('data/text/test.normalized.txt')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "write_phones(Path(\"./data/text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Encodec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from functools import cache\n",
    "import torchaudio\n",
    "from encodec import EncodecModel\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "import soundfile\n",
    "from encodec.utils import convert_audio\n",
    "\n",
    "SAMPLE_RATE = 24_000\n",
    "BANDWIDTHS  = [1.5, 3.0, 6.0, 12.0, 24.0]\n",
    "BANDWIDTH   = BANDWIDTHS[0]\n",
    "\n",
    "@cache\n",
    "def _load_model(bandwidth=6.0, device=\"cuda\"):\n",
    "    # Instantiate a pretrained EnCodec model\n",
    "    assert SAMPLE_RATE == 24_000\n",
    "    model = EncodecModel.encodec_model_24khz()\n",
    "    model.set_target_bandwidth(bandwidth)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def unload_model():\n",
    "    return _load_model.cache_clear()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def decode(codes: Tensor, bandwidth=6.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        codes: (b q t)\n",
    "    \"\"\"\n",
    "    assert codes.dim() == 3\n",
    "    model = _load_model(bandwidth, device)\n",
    "    return model.decode([(codes, None)]), model.sample_rate\n",
    "\n",
    "def decode_to_file(resps: Tensor, path: Path):\n",
    "    assert resps.dim() == 2, f\"Require shape (t q), but got {resps.shape}.\"\n",
    "    resps = rearrange(resps, \"t q -> 1 q t\")\n",
    "    wavs, sr = decode(codes=resps, bandwidth=BANDWIDTH)\n",
    "    soundfile.write(str(path), wavs.cpu()[0, 0], sr)\n",
    "\n",
    "def _replace_file_extension(path, suffix):\n",
    "    return (path.parent / path.name.split(\".\")[0]).with_suffix(suffix)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode(wav: Tensor, sr: int, bandwidth=6.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        wav: (t)\n",
    "        sr: int\n",
    "    \"\"\"\n",
    "    model = _load_model(bandwidth, device)\n",
    "    wav = wav.unsqueeze(0)\n",
    "    wav = convert_audio(wav, sr, model.sample_rate, model.channels)\n",
    "    wav = wav.to(device)\n",
    "    encoded_frames = model.encode(wav)\n",
    "    qnt = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # (b q t)\n",
    "    return qnt\n",
    "\n",
    "def encode_from_file(path, bandwidth=6.0, device=\"cuda\"):\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    if wav.shape[0] == 2:\n",
    "        wav = wav[:1]\n",
    "    return encode(wav, sr, bandwidth, device)\n",
    "\n",
    "def quantize_audio(folder, suffix=\".wav\"):\n",
    "    paths = [*folder.rglob(f\"*{suffix}\")]\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        out_path = _replace_file_extension(path, \".qnt.pt\")\n",
    "        if out_path.exists():\n",
    "            continue\n",
    "        qnt = encode_from_file(path, BANDWIDTH)\n",
    "        print(qnt.shape)\n",
    "        torch.save(qnt.cpu(), out_path)\n",
    "\n",
    "def decode_files(folder, suffix=\".qnt.pt\"):\n",
    "    paths = [*folder.rglob(f\"*{suffix}\")]\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        out_path = _replace_file_extension(path, \".qt.wav\")\n",
    "        if out_path.exists():\n",
    "            continue\n",
    "        fi = rearrange(torch.load(path).squeeze(0).cuda(), \"q t -> t q\")\n",
    "        decode_to_file(fi, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "quantize_audio(Path(\"./data/audio\"))\n",
    "decode_files(Path(\"./data/audio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"data/audio/test.qnt.pt\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LJSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDWIDTH_IDX = 0\n",
    "CODEBOOKS     = [2, 4, 8, 16, 32]\n",
    "BANDWIDTHS    = [1.5, 3.0, 6.0, 12.0, 24.0]\n",
    "BANDWIDTH     = BANDWIDTHS[BANDWIDTH_IDX]\n",
    "CODEBOOK      = CODEBOOKS[BANDWIDTH_IDX]\n",
    "\n",
    "import torchaudio\n",
    "from ljspeech import LJSPEECH\n",
    "DATASET_PATH = \"./data/LJSpeech/\"\n",
    "dataset = LJSPEECH(\n",
    "    \"./data/LJSpeech\",\n",
    "    encodec_bandwidth=BANDWIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 725])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler, collate_fn=lambda x: x)\n",
    "test_loader = DataLoader(dataset, batch_size=32, sampler=test_sampler, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 41)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import megabyte\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "def get_reserved_mem_gb():\n",
    "    device = torch.cuda.current_device()\n",
    "    reserved = torch.cuda.memory_reserved(device)\n",
    "    reserved_gb = reserved / 1024 / 1024 / 1024\n",
    "    return reserved_gb\n",
    "\n",
    "class PhoneLM(nn.Module):\n",
    "    def __init__(self, n_phone_tokens, n_audio_tokens):\n",
    "        super(PhoneLM, self).__init__()\n",
    "        self.megabyte   = megabyte.MEGABYTE(\n",
    "            heads       = 8, # 1,\n",
    "            dim_head    = 32, # 16,\n",
    "            num_tokens  = n_phone_tokens + n_audio_tokens + 4,\n",
    "            dim         = (768, 256, 128), # (32, 32, 32), # (768, 256, 128)# Dg, Dl1, Dl2\n",
    "            depth       = (6, 4, 2), # (6, 4, 2)\n",
    "            max_seq_len = (32, 4, 4),\n",
    "            flash_attn  = False)\n",
    "\n",
    "    def forward(self, x, debug=False, return_loss=True):\n",
    "        x = self.megabyte(x, return_loss=return_loss)\n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        o = [param.numel() for param in self.parameters() if param.requires_grad]\n",
    "        o = sum(o)\n",
    "        return o\n",
    "    \n",
    "    def generate(self, *args):\n",
    "        return self.megabyte.generate(*args)\n",
    "    \n",
    "def multi_encode(\n",
    "        phone_tokens,\n",
    "        audio_tokens,\n",
    "        n_phone_tokens,\n",
    "        n_audio_tokens,\n",
    "        max_clip_length=1.0):\n",
    "    \"\"\"NOTE: 75 steps per second for 24kHz in `encodec.\n",
    "    Set `max_clip_length` to 0 for original clip length.\"\"\"\n",
    "\n",
    "    # Start text token, end text token, start audio token, end audio token\n",
    "    STT, ETT, SAT, EAT = [n_phone_tokens + n_audio_tokens + i\n",
    "                          for i in range(4)]\n",
    "    STT = torch.tensor([STT]).long().cuda()\n",
    "    ETT = torch.tensor([ETT]).long().cuda()\n",
    "    SAT = torch.tensor([SAT]).long().cuda()\n",
    "    EAT = torch.tensor([EAT]).long().cuda()\n",
    "\n",
    "    if max_clip_length:\n",
    "        audio_tokens = audio_tokens[:, :, :int(max_clip_length * 75)]\n",
    "    audio_tokens = rearrange(audio_tokens.squeeze(0), \"q s -> (q s)\")\n",
    "    \n",
    "    # offset phone tokens past audio tokens\n",
    "    phone_tokens += n_audio_tokens\n",
    "    \n",
    "    print(\"phone_tokens.shape:\", phone_tokens.shape)\n",
    "    print(\"audio_tokens.shape:\", audio_tokens.shape)\n",
    "    \n",
    "    device = torch.cuda.current_device()\n",
    "    phone_tokens = torch.cat((STT, phone_tokens, ETT), dim=0).to(device)\n",
    "    audio_tokens = torch.cat((SAT, audio_tokens, EAT,), dim=0).to(device)\n",
    "    combined_tokens = torch.cat((phone_tokens, audio_tokens), dim=0).to(device)\n",
    "    return phone_tokens, audio_tokens, combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "from encodec_util import decode_to_file\n",
    "\n",
    "def generate_audio(sample,\n",
    "                   n_phone_tokens,\n",
    "                   n_audio_tokens,\n",
    "                   audio_path=\"./out.wav\"):\n",
    "    STT, ETT, SAT, EAT = [n_phone_tokens + n_audio_tokens + i\n",
    "                          for i in range(4)]\n",
    "    ST_S = [STT, ETT, SAT, EAT]\n",
    "    print(\"STT, ETT, SAT, EAT ids:\", ST_S)\n",
    "    seq = sample.cpu().tolist()[0]\n",
    "    print(\"seq:\", seq)\n",
    "    # all special tokens in list\n",
    "    if all(st_t in seq for st_t in ST_S) and len(seq) >= len(ST_S) + 2:\n",
    "        # text_tokens  = seq[seq.index(STT + 1):seq.index(ETT - 1)]\n",
    "        audio_tokens = seq[seq.index(SAT)+1:seq.index(EAT)]\n",
    "        print(seq.index(SAT), seq.index(EAT), len(audio_tokens))\n",
    "        audio_tokens = torch.tensor(audio_tokens).cuda()\n",
    "        audio_tokens = rearrange(\n",
    "            audio_tokens,\n",
    "            '(t q) -> t q',\n",
    "            q=CODEBOOK,\n",
    "            t=audio_tokens.size(0) // CODEBOOK)\n",
    "        print(\"audio_tokens.shape:\", audio_tokens, audio_tokens.shape)\n",
    "        decode_to_file(audio_tokens, audio_path)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhoneLM - LJSpeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 37.30M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37302863"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PhoneLM(\n",
    "    n_phone_tokens=len(dataset.phone_dict),\n",
    "    n_audio_tokens=1024).to(device)\n",
    "\n",
    "model.megabyte.get_num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([121]), torch.Size([1, 2, 660]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(train_loader))[0]\n",
    "item_phone_tokens = item[-2]\n",
    "item_audio_tokens = item[-1]\n",
    "item_phone_tokens.shape, item_audio_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/LJSpeech/LJSpeech-1.1/wavs/LJ016-0073.wav'),\n",
       " tensor([[-9.1553e-05,  0.0000e+00,  9.1553e-05,  ..., -6.1035e-05,\n",
       "          -5.4932e-04, -2.4414e-04]]),\n",
       " 22050,\n",
       " 'Mr. Cope, the governor of Newgate, having been communicated with, proceeded to Winchester, where he at once identified Williams.',\n",
       " 'Mr. Cope, the governor of Newgate, having been communicated with, proceeded to Winchester, where he at once identified Williams.',\n",
       " ['M',\n",
       "  'IH1',\n",
       "  'S',\n",
       "  'T',\n",
       "  'ER0',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'K',\n",
       "  'OW1',\n",
       "  'P',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'DH',\n",
       "  'AH0',\n",
       "  '_',\n",
       "  'G',\n",
       "  'AH1',\n",
       "  'V',\n",
       "  'ER0',\n",
       "  'N',\n",
       "  'ER0',\n",
       "  '_',\n",
       "  'AH1',\n",
       "  'V',\n",
       "  '_',\n",
       "  'N',\n",
       "  'UW1',\n",
       "  'G',\n",
       "  'EY0',\n",
       "  'T',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'HH',\n",
       "  'AE1',\n",
       "  'V',\n",
       "  'IH0',\n",
       "  'NG',\n",
       "  '_',\n",
       "  'B',\n",
       "  'IH1',\n",
       "  'N',\n",
       "  '_',\n",
       "  'K',\n",
       "  'AH0',\n",
       "  'M',\n",
       "  'Y',\n",
       "  'UW1',\n",
       "  'N',\n",
       "  'AH0',\n",
       "  'K',\n",
       "  'EY2',\n",
       "  'T',\n",
       "  'IH0',\n",
       "  'D',\n",
       "  '_',\n",
       "  'W',\n",
       "  'IH1',\n",
       "  'DH',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'P',\n",
       "  'R',\n",
       "  'AH0',\n",
       "  'S',\n",
       "  'IY1',\n",
       "  'D',\n",
       "  'AH0',\n",
       "  'D',\n",
       "  '_',\n",
       "  'T',\n",
       "  'UW1',\n",
       "  '_',\n",
       "  'W',\n",
       "  'IH1',\n",
       "  'N',\n",
       "  'CH',\n",
       "  'EH2',\n",
       "  'S',\n",
       "  'T',\n",
       "  'ER0',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'W',\n",
       "  'EH1',\n",
       "  'R',\n",
       "  '_',\n",
       "  'HH',\n",
       "  'IY1',\n",
       "  '_',\n",
       "  'AE1',\n",
       "  'T',\n",
       "  '_',\n",
       "  'W',\n",
       "  'AH1',\n",
       "  'N',\n",
       "  'S',\n",
       "  '_',\n",
       "  'AY0',\n",
       "  'D',\n",
       "  'EH1',\n",
       "  'N',\n",
       "  'T',\n",
       "  'AH0',\n",
       "  'F',\n",
       "  'AY2',\n",
       "  'D',\n",
       "  '_',\n",
       "  'W',\n",
       "  'IH1',\n",
       "  'L',\n",
       "  'Y',\n",
       "  'AH0',\n",
       "  'M',\n",
       "  'Z',\n",
       "  '_',\n",
       "  '_'],\n",
       " tensor([1071, 1063, 1082, 1084, 1053, 1098, 1098, 1098, 1069, 1075, 1080, 1098,\n",
       "         1098, 1098, 1049, 1034, 1098, 1060, 1035, 1093, 1053, 1072, 1053, 1098,\n",
       "         1035, 1093, 1098, 1072, 1091, 1060, 1056, 1084, 1098, 1098, 1098, 1061,\n",
       "         1032, 1093, 1062, 1073, 1098, 1046, 1063, 1072, 1098, 1069, 1034, 1071,\n",
       "         1095, 1091, 1072, 1034, 1069, 1058, 1084, 1062, 1048, 1098, 1094, 1063,\n",
       "         1049, 1098, 1098, 1098, 1080, 1081, 1034, 1082, 1066, 1048, 1034, 1048,\n",
       "         1098, 1084, 1091, 1098, 1094, 1063, 1072, 1047, 1052, 1082, 1084, 1053,\n",
       "         1098, 1098, 1098, 1094, 1051, 1081, 1098, 1061, 1066, 1098, 1032, 1084,\n",
       "         1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084, 1034,\n",
       "         1059, 1045, 1048, 1098, 1094, 1063, 1070, 1095, 1034, 1071, 1096, 1098,\n",
       "         1098], device='cuda:0'),\n",
       " tensor([[[408, 976, 860,  ..., 106, 835, 408],\n",
       "          [913, 877, 252,  ..., 518, 518, 424]]], device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phone_tokens.shape: torch.Size([121])\n",
      "audio_tokens.shape: torch.Size([150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([275])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_prompt, audio_target, test_inp = multi_encode(\n",
    "    item_phone_tokens,\n",
    "    item_audio_tokens,\n",
    "    n_phone_tokens=len(dataset.phone_dict),\n",
    "    n_audio_tokens=1024)\n",
    "test_inp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "MAX_LR       = 1e-2\n",
    "# MAX_LR       = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GRAD_CLIP    = 0.1\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=MAX_LR)\n",
    "    #,weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# def get_lr(optimizer):\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         return param_group['lr']\n",
    "\n",
    "# sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, MAX_LR, epochs=epochs, \n",
    "#                                                 steps_per_epoch=len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1099, 1071, 1063, 1082, 1084, 1053, 1098, 1098, 1098, 1069, 1075, 1080,\n",
       "        1098, 1098, 1098, 1049, 1034, 1098, 1060, 1035, 1093, 1053, 1072, 1053,\n",
       "        1098, 1035, 1093, 1098, 1072, 1091, 1060, 1056, 1084, 1098, 1098, 1098,\n",
       "        1061, 1032, 1093, 1062, 1073, 1098, 1046, 1063, 1072, 1098, 1069, 1034,\n",
       "        1071, 1095, 1091, 1072, 1034, 1069, 1058, 1084, 1062, 1048, 1098, 1094,\n",
       "        1063, 1049, 1098, 1098, 1098, 1080, 1081, 1034, 1082, 1066, 1048, 1034,\n",
       "        1048, 1098, 1084, 1091, 1098, 1094, 1063, 1072, 1047, 1052, 1082, 1084,\n",
       "        1053, 1098, 1098, 1098, 1094, 1051, 1081, 1098, 1061, 1066, 1098, 1032,\n",
       "        1084, 1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084,\n",
       "        1034, 1059, 1045, 1048, 1098, 1094, 1063, 1070, 1095, 1034, 1071, 1096,\n",
       "        1098, 1098, 1100, 1101,  408,  976,  860,  388,  540,  901,  373,  574,\n",
       "         574,  574,   47,  574,  148,  738,  339,  176,  254,  103,  862,  958,\n",
       "         612, 1011,  472,  475,  475,  779,  855,  835,  835,  106,  405,  213,\n",
       "        1014,  798,  537,  887,  575,  575,  504,  288,  755,  259,  837,  291,\n",
       "         808,  942,  921,  291,  155,  523,   52,   52,  370,   52,  106,  370,\n",
       "         257,  257,  904,  537,  395,  408,  404,  106,  855,  475,  738,  738,\n",
       "         738,  408,  738,  106,  408,  408,  408,  913,  877,  252,  418,  792,\n",
       "         674,  991,  160, 1010,  214,  209,  765,  652,  652,  646,  870, 1010,\n",
       "         420,  624,  486,  948,  948,  419,  200,  580,  913,  700,  913,  424,\n",
       "         544,  632,  872,  292,  863,  729,  384,  146,  563,  889,  499,  599,\n",
       "         751,  684,  668,  283,  379,   30,  593,  128,  230,  687,  984,  984,\n",
       "         928,  913,  924,  942,  913,  969,  387,  921,  928, 1007, 1007, 1007,\n",
       "        1007,  544,  913,  518,  424,  518,  913,  424,  424,  424, 1102],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([275])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved Memory (GB): 0.99609375, loss: 7.006810188293457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved Memory (GB): 1.017578125, loss: 4.011221408843994\n",
      "Reserved Memory (GB): 1.017578125, loss: 0.15099826455116272\n",
      "Reserved Memory (GB): 1.017578125, loss: 0.04681147634983063\n",
      "Reserved Memory (GB): 1.017578125, loss: 0.016797270625829697\n",
      "Reserved Memory (GB): 1.017578125, loss: 0.016020676121115685\n",
      "Reserved Memory (GB): 1.017578125, loss: 0.016087962314486504\n",
      "Reserved Memory (GB): 1.017578125, loss: 0.01562683843076229\n",
      "Reserved Memory (GB): 1.017578125, loss: 0.015517176128923893\n",
      "Reserved Memory (GB): 1.017578125, loss: 0.015402528457343578\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "EPOCHS = 1000\n",
    "PRINT_INTERVAL = 100\n",
    "\n",
    "seq_len = 512\n",
    "\n",
    "def train(model, trainloader):\n",
    "    model.train()\n",
    "    \n",
    "    padding_len = max(0, seq_len - test_inp.size(0))\n",
    "    n_test_inp = F.pad(test_inp, (0, padding_len))\n",
    "    batch = n_test_inp.unsqueeze(0)\n",
    "    # print(batch.shape)\n",
    "    loss = model(batch, return_loss=True)\n",
    "    # loss = model(next(trainloader), return_loss=True)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# pbar = tqdm.tqdm(EPOCHS, mininterval=10., desc='training')\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(model, train_loader)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    mem_gb = get_reserved_mem_gb()\n",
    "    if epoch % PRINT_INTERVAL == 0:\n",
    "        print(f\"Reserved Memory (GB): {mem_gb}, loss: {loss.item()}\")\n",
    "    #' pbar.set_description(f\"Reserved Memory (GB): {mem_gb}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1099, 1071, 1063, 1082, 1084, 1053, 1098, 1098, 1098, 1069, 1075, 1080,\n",
       "        1098, 1098, 1098, 1049, 1034, 1098, 1060, 1035, 1093, 1053, 1072, 1053,\n",
       "        1098, 1035, 1093, 1098, 1072, 1091, 1060, 1056, 1084, 1098, 1098, 1098,\n",
       "        1061, 1032, 1093, 1062, 1073, 1098, 1046, 1063, 1072, 1098, 1069, 1034,\n",
       "        1071, 1095, 1091, 1072, 1034, 1069, 1058, 1084, 1062, 1048, 1098, 1094,\n",
       "        1063, 1049, 1098, 1098, 1098, 1080, 1081, 1034, 1082, 1066, 1048, 1034,\n",
       "        1048, 1098, 1084, 1091, 1098, 1094, 1063, 1072, 1047, 1052, 1082, 1084,\n",
       "        1053, 1098, 1098, 1098, 1094, 1051, 1081, 1098, 1061, 1066, 1098, 1032,\n",
       "        1084, 1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084,\n",
       "        1034, 1059, 1045, 1048, 1098, 1094, 1063, 1070, 1095, 1034, 1071, 1096,\n",
       "        1098, 1098, 1100], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1101,  408,  976,  860,  388,  540,  901,  373,  574,  574,  574,   47,\n",
       "         574,  148,  738,  339,  176,  254,  103,  862,  958,  612, 1011,  472,\n",
       "         475,  475,  779,  855,  835,  835,  106,  405,  213, 1014,  798,  537,\n",
       "         887,  575,  575,  504,  288,  755,  259,  837,  291,  808,  942,  921,\n",
       "         291,  155,  523,   52,   52,  370,   52,  106,  370,  257,  257,  904,\n",
       "         537,  395,  408,  404,  106,  855,  475,  738,  738,  738,  408,  738,\n",
       "         106,  408,  408,  408,  913,  877,  252,  418,  792,  674,  991,  160,\n",
       "        1010,  214,  209,  765,  652,  652,  646,  870, 1010,  420,  624,  486,\n",
       "         948,  948,  419,  200,  580,  913,  700,  913,  424,  544,  632,  872,\n",
       "         292,  863,  729,  384,  146,  563,  889,  499,  599,  751,  684,  668,\n",
       "         283,  379,   30,  593,  128,  230,  687,  984,  984,  928,  913,  924,\n",
       "         942,  913,  969,  387,  921,  928, 1007, 1007, 1007, 1007,  544,  913,\n",
       "         518,  424,  518,  913,  424,  424,  424, 1102], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 389/389 [00:05<00:00, 75.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: tensor([[1099, 1071, 1063, 1082, 1084, 1053, 1098, 1098, 1098, 1069, 1075, 1080,\n",
      "         1098, 1098, 1098, 1049, 1034, 1098, 1060, 1035, 1093, 1053, 1072, 1053,\n",
      "         1098, 1035, 1093, 1098, 1072, 1091, 1060, 1056, 1084, 1098, 1098, 1098,\n",
      "         1061, 1032, 1093, 1062, 1073, 1098, 1046, 1063, 1072, 1098, 1069, 1034,\n",
      "         1071, 1095, 1091, 1072, 1034, 1069, 1058, 1084, 1062, 1048, 1098, 1094,\n",
      "         1063, 1049, 1098, 1098, 1098, 1080, 1081, 1034, 1082, 1066, 1048, 1034,\n",
      "         1048, 1098, 1084, 1091, 1098, 1094, 1063, 1072, 1047, 1052, 1082, 1084,\n",
      "         1053, 1098, 1098, 1098, 1094, 1051, 1081, 1098, 1061, 1066, 1098, 1032,\n",
      "         1084, 1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084,\n",
      "         1034, 1059, 1045, 1048, 1098, 1094, 1063, 1070, 1095, 1034, 1071, 1096,\n",
      "         1098, 1098, 1100, 1101,  408,  976,  860,  388,  540,  901,  373,  574,\n",
      "          574,  574,   47,  574,  148,  738,  339,  176,  254,  103,  862,  958,\n",
      "          612, 1011,  472,  475,  475,  779,  855,  835,  835,  106,  405,  213,\n",
      "         1014,  798,  537,  887,  575,  575,  504,  288,  755,  259,  837,  291,\n",
      "          808,  942,  921,  291,  155,  523,   52,   52,  370,   52,  106,  370,\n",
      "          257,  257,  904,  537,  395,  408,  404,  106,  855,  475,  738,  738,\n",
      "          738,  408,  738,  106,  408,  408,  408,  913,  877,  252,  418,  792,\n",
      "          674,  991,  160, 1010,  214,  209,  765,  652,  652,  646,  870, 1010,\n",
      "          420,  624,  486,  948,  948,  419,  200,  580,  913,  700,  913,  424,\n",
      "          544,  632,  872,  292,  863,  729,  384,  146,  563,  889,  499,  599,\n",
      "          751,  684,  668,  283,  379,   30,  593,  128,  230,  687,  984,  984,\n",
      "          928,  913,  924,  942,  913,  969,  387,  921,  928, 1007, 1007, 1007,\n",
      "         1007,  544,  913,  518,  424,  518,  913,  424,  424,  424, 1102, 1082,\n",
      "         1084, 1053, 1098, 1098, 1098, 1035, 1093, 1098, 1072, 1091, 1060, 1056,\n",
      "         1084, 1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084,\n",
      "         1034, 1059, 1045, 1048, 1098, 1080, 1081, 1034, 1082, 1066, 1048, 1034,\n",
      "         1048, 1098, 1084, 1091, 1098, 1094, 1063, 1072, 1047, 1052, 1082, 1084,\n",
      "         1053, 1098, 1098, 1098, 1094, 1051, 1081, 1098, 1061, 1066, 1098, 1032,\n",
      "         1084, 1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084,\n",
      "         1034, 1059, 1045, 1048, 1098, 1094, 1063, 1070, 1095, 1034, 1071, 1096,\n",
      "         1098, 1098, 1100, 1101,  408,  976,  860,  388,  540,  901,  373,  574,\n",
      "          574,  574,   47,  574,  148,  738,  339,  176,  254,  103,  862,  958,\n",
      "          612, 1011,  472,  475,  475,  779,  855,  835,  835,  106,  405,  213,\n",
      "         1014,  798,  537,  887,  575,  575,  504,  288,  755,  259,  837,  291,\n",
      "          808,  942,  921,  291,  155,  523,   52,   52,  370,   52,  106,  370,\n",
      "          257,  257,  904,  537,  395,  408,  404,  106,  855,  475,  738,  738,\n",
      "          738,  408,  738,  106,  408,  408,  408,  913,  877,  252,  418,  792,\n",
      "          674,  991,  160, 1010,  214,  209,  765,  652,  652,  646,  870, 1010,\n",
      "          420,  624,  486,  948,  948,  419,  200,  580,  913,  700,  913,  424,\n",
      "          544,  632,  872,  292,  863,  729,  384,  146,  563,  889,  499,  599,\n",
      "          751,  684,  668,  283,  379,   30,  593,  128,  230,  687,  984,  984,\n",
      "          928,  913,  924,  942,  913,  700,  913,  424,  544,  632,  872,  292,\n",
      "          863,  729,  384,  146,  563,  889,  499,  599]], device='cuda:0') torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate(model, prompt):\n",
    "    model.eval()\n",
    "\n",
    "    prompt = prompt.unsqueeze(0)\n",
    "    sample = model.generate(prompt)\n",
    "    sample = sample.flatten(1)\n",
    "    print(\"sample:\", sample, sample.shape)\n",
    "\n",
    "    return prompt, sample\n",
    "\n",
    "prompt, sample = generate(model, phone_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STT, ETT, SAT, EAT ids: [1099, 1100, 1101, 1102]\n",
      "seq: [1099, 1071, 1063, 1082, 1084, 1053, 1098, 1098, 1098, 1069, 1075, 1080, 1098, 1098, 1098, 1049, 1034, 1098, 1060, 1035, 1093, 1053, 1072, 1053, 1098, 1035, 1093, 1098, 1072, 1091, 1060, 1056, 1084, 1098, 1098, 1098, 1061, 1032, 1093, 1062, 1073, 1098, 1046, 1063, 1072, 1098, 1069, 1034, 1071, 1095, 1091, 1072, 1034, 1069, 1058, 1084, 1062, 1048, 1098, 1094, 1063, 1049, 1098, 1098, 1098, 1080, 1081, 1034, 1082, 1066, 1048, 1034, 1048, 1098, 1084, 1091, 1098, 1094, 1063, 1072, 1047, 1052, 1082, 1084, 1053, 1098, 1098, 1098, 1094, 1051, 1081, 1098, 1061, 1066, 1098, 1032, 1084, 1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084, 1034, 1059, 1045, 1048, 1098, 1094, 1063, 1070, 1095, 1034, 1071, 1096, 1098, 1098, 1100, 1101, 408, 976, 860, 388, 540, 901, 373, 574, 574, 574, 47, 574, 148, 738, 339, 176, 254, 103, 862, 958, 612, 1011, 472, 475, 475, 779, 855, 835, 835, 106, 405, 213, 1014, 798, 537, 887, 575, 575, 504, 288, 755, 259, 837, 291, 808, 942, 921, 291, 155, 523, 52, 52, 370, 52, 106, 370, 257, 257, 904, 537, 395, 408, 404, 106, 855, 475, 738, 738, 738, 408, 738, 106, 408, 408, 408, 913, 877, 252, 418, 792, 674, 991, 160, 1010, 214, 209, 765, 652, 652, 646, 870, 1010, 420, 624, 486, 948, 948, 419, 200, 580, 913, 700, 913, 424, 544, 632, 872, 292, 863, 729, 384, 146, 563, 889, 499, 599, 751, 684, 668, 283, 379, 30, 593, 128, 230, 687, 984, 984, 928, 913, 924, 942, 913, 969, 387, 921, 928, 1007, 1007, 1007, 1007, 544, 913, 518, 424, 518, 913, 424, 424, 424, 1102, 1082, 1084, 1053, 1098, 1098, 1098, 1035, 1093, 1098, 1072, 1091, 1060, 1056, 1084, 1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084, 1034, 1059, 1045, 1048, 1098, 1080, 1081, 1034, 1082, 1066, 1048, 1034, 1048, 1098, 1084, 1091, 1098, 1094, 1063, 1072, 1047, 1052, 1082, 1084, 1053, 1098, 1098, 1098, 1094, 1051, 1081, 1098, 1061, 1066, 1098, 1032, 1084, 1098, 1094, 1035, 1072, 1082, 1098, 1043, 1048, 1051, 1072, 1084, 1034, 1059, 1045, 1048, 1098, 1094, 1063, 1070, 1095, 1034, 1071, 1096, 1098, 1098, 1100, 1101, 408, 976, 860, 388, 540, 901, 373, 574, 574, 574, 47, 574, 148, 738, 339, 176, 254, 103, 862, 958, 612, 1011, 472, 475, 475, 779, 855, 835, 835, 106, 405, 213, 1014, 798, 537, 887, 575, 575, 504, 288, 755, 259, 837, 291, 808, 942, 921, 291, 155, 523, 52, 52, 370, 52, 106, 370, 257, 257, 904, 537, 395, 408, 404, 106, 855, 475, 738, 738, 738, 408, 738, 106, 408, 408, 408, 913, 877, 252, 418, 792, 674, 991, 160, 1010, 214, 209, 765, 652, 652, 646, 870, 1010, 420, 624, 486, 948, 948, 419, 200, 580, 913, 700, 913, 424, 544, 632, 872, 292, 863, 729, 384, 146, 563, 889, 499, 599, 751, 684, 668, 283, 379, 30, 593, 128, 230, 687, 984, 984, 928, 913, 924, 942, 913, 700, 913, 424, 544, 632, 872, 292, 863, 729, 384, 146, 563, 889, 499, 599]\n",
      "123 274 150\n",
      "audio_tokens.shape: tensor([[ 408,  976],\n",
      "        [ 860,  388],\n",
      "        [ 540,  901],\n",
      "        [ 373,  574],\n",
      "        [ 574,  574],\n",
      "        [  47,  574],\n",
      "        [ 148,  738],\n",
      "        [ 339,  176],\n",
      "        [ 254,  103],\n",
      "        [ 862,  958],\n",
      "        [ 612, 1011],\n",
      "        [ 472,  475],\n",
      "        [ 475,  779],\n",
      "        [ 855,  835],\n",
      "        [ 835,  106],\n",
      "        [ 405,  213],\n",
      "        [1014,  798],\n",
      "        [ 537,  887],\n",
      "        [ 575,  575],\n",
      "        [ 504,  288],\n",
      "        [ 755,  259],\n",
      "        [ 837,  291],\n",
      "        [ 808,  942],\n",
      "        [ 921,  291],\n",
      "        [ 155,  523],\n",
      "        [  52,   52],\n",
      "        [ 370,   52],\n",
      "        [ 106,  370],\n",
      "        [ 257,  257],\n",
      "        [ 904,  537],\n",
      "        [ 395,  408],\n",
      "        [ 404,  106],\n",
      "        [ 855,  475],\n",
      "        [ 738,  738],\n",
      "        [ 738,  408],\n",
      "        [ 738,  106],\n",
      "        [ 408,  408],\n",
      "        [ 408,  913],\n",
      "        [ 877,  252],\n",
      "        [ 418,  792],\n",
      "        [ 674,  991],\n",
      "        [ 160, 1010],\n",
      "        [ 214,  209],\n",
      "        [ 765,  652],\n",
      "        [ 652,  646],\n",
      "        [ 870, 1010],\n",
      "        [ 420,  624],\n",
      "        [ 486,  948],\n",
      "        [ 948,  419],\n",
      "        [ 200,  580],\n",
      "        [ 913,  700],\n",
      "        [ 913,  424],\n",
      "        [ 544,  632],\n",
      "        [ 872,  292],\n",
      "        [ 863,  729],\n",
      "        [ 384,  146],\n",
      "        [ 563,  889],\n",
      "        [ 499,  599],\n",
      "        [ 751,  684],\n",
      "        [ 668,  283],\n",
      "        [ 379,   30],\n",
      "        [ 593,  128],\n",
      "        [ 230,  687],\n",
      "        [ 984,  984],\n",
      "        [ 928,  913],\n",
      "        [ 924,  942],\n",
      "        [ 913,  969],\n",
      "        [ 387,  921],\n",
      "        [ 928, 1007],\n",
      "        [1007, 1007],\n",
      "        [1007,  544],\n",
      "        [ 913,  518],\n",
      "        [ 424,  518],\n",
      "        [ 913,  424],\n",
      "        [ 424,  424]], device='cuda:0') torch.Size([75, 2])\n"
     ]
    }
   ],
   "source": [
    "out = generate_audio(\n",
    "    sample,\n",
    "    n_phone_tokens=len(dataset.phone_dict),\n",
    "    n_audio_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
