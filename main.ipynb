{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhoneLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `G2P` and `Encodec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install g2p_en encodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `G2P`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import string\n",
    "from functools import cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "@cache\n",
    "def _get_model():\n",
    "    return G2p()\n",
    "\n",
    "@cache\n",
    "def _get_graphs(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        graphs = f.read()\n",
    "    return graphs\n",
    "\n",
    "def encode(graphs: str) -> list[str]:\n",
    "    g2p = _get_model()\n",
    "    phones = g2p(graphs)\n",
    "    ignored = {\" \", *string.punctuation}\n",
    "    return [\"_\" if p in ignored else p for p in phones]\n",
    "\n",
    "@torch.no_grad()\n",
    "def write_phones(folder, suffix=\".normalized.txt\"):\n",
    "    print(\"ello?\")\n",
    "    paths = list(folder.rglob(f\"*{suffix}\"))\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    print(\"paths:\", paths)\n",
    "    for path in tqdm(paths):\n",
    "        phone_path = path.with_name(path.stem.split(\".\")[0] + \".phn.txt\")\n",
    "        if phone_path.exists():\n",
    "            continue\n",
    "        print(\"?\")\n",
    "        graphs = _get_graphs(path)\n",
    "        phones = encode(graphs)\n",
    "        with open(phone_path, \"w\") as f:\n",
    "            f.write(\" \".join(phones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ello?\n",
      "paths: [WindowsPath('data/text/test.normalized.txt')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "write_phones(Path(\"./data/text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Encodec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from functools import cache\n",
    "import torchaudio\n",
    "from encodec import EncodecModel\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "import soundfile\n",
    "from encodec.utils import convert_audio\n",
    "from pathlib import Path\n",
    "\n",
    "SAMPLE_RATE = 24_000\n",
    "BANDWIDTHS  = [1.5, 3.0, 6.0, 12.0, 24.0]\n",
    "BANDWIDTH   = BANDWIDTHS[0]\n",
    "\n",
    "@cache\n",
    "def _load_model(bandwidth=6.0, device=\"cuda\"):\n",
    "    # Instantiate a pretrained EnCodec model\n",
    "    assert SAMPLE_RATE == 24_000\n",
    "    model = EncodecModel.encodec_model_24khz()\n",
    "    model.set_target_bandwidth(bandwidth)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def unload_model():\n",
    "    return _load_model.cache_clear()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def decode(codes: Tensor, bandwidth=6.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        codes: (b q t)\n",
    "    \"\"\"\n",
    "    assert codes.dim() == 3\n",
    "    model = _load_model(bandwidth, device)\n",
    "    return model.decode([(codes, None)]), model.sample_rate\n",
    "\n",
    "def decode_to_file(resps: Tensor, path: Path):\n",
    "    assert resps.dim() == 2, f\"Require shape (t q), but got {resps.shape}.\"\n",
    "    resps = rearrange(resps, \"t q -> 1 q t\")\n",
    "    wavs, sr = decode(codes=resps, bandwidth=BANDWIDTH)\n",
    "    soundfile.write(str(path), wavs.cpu()[0, 0], sr)\n",
    "\n",
    "def _replace_file_extension(path, suffix):\n",
    "    return (path.parent / path.name.split(\".\")[0]).with_suffix(suffix)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode(wav: Tensor, sr: int, bandwidth=6.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        wav: (t)\n",
    "        sr: int\n",
    "    \"\"\"\n",
    "    model = _load_model(bandwidth, device)\n",
    "    wav = wav.unsqueeze(0)\n",
    "    wav = convert_audio(wav, sr, model.sample_rate, model.channels)\n",
    "    wav = wav.to(device)\n",
    "    encoded_frames = model.encode(wav)\n",
    "    qnt = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # (b q t)\n",
    "    return qnt\n",
    "\n",
    "def encode_from_file(path, bandwidth=6.0, device=\"cuda\"):\n",
    "    wav, sr = torchaudio.load(str(path))\n",
    "    if wav.shape[0] == 2:\n",
    "        wav = wav[:1]\n",
    "    return encode(wav, sr, bandwidth, device)\n",
    "\n",
    "def quantize_audio(folder, suffix=\".wav\"):\n",
    "    paths = [*folder.rglob(f\"*{suffix}\")]\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        out_path = _replace_file_extension(path, \".qnt.pt\")\n",
    "        if out_path.exists():\n",
    "            continue\n",
    "        qnt = encode_from_file(path, BANDWIDTH)\n",
    "        print(qnt.shape)\n",
    "        torch.save(qnt.cpu(), out_path)\n",
    "\n",
    "def decode_files(folder, suffix=\".qnt.pt\"):\n",
    "    paths = [*folder.rglob(f\"*{suffix}\")]\n",
    "    random.shuffle(paths)\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        out_path = _replace_file_extension(path, \".qt.wav\")\n",
    "        if out_path.exists():\n",
    "            continue\n",
    "        fi = rearrange(torch.load(path).squeeze(0).cuda(), \"q t -> t q\")\n",
    "        decode_to_file(fi, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# quantize_audio(Path(\"./data/audio\"))\n",
    "# decode_files(Path(\"./data/audio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(\"data/audio/test.qnt.pt\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Audio from Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor = torch.tensor([[1019,  662],\n",
    "        [ 598,   25],\n",
    "        [ 321,  463],\n",
    "        [1063,  575],\n",
    "        [ 745,  727],\n",
    "        [1073,  344],\n",
    "        [1098,  344],\n",
    "        [1046,  959],\n",
    "        [1062,  874],\n",
    "        [1059,  804],\n",
    "        [1038, 1010],\n",
    "        [1081,  577],\n",
    "        [1098,  323],\n",
    "        [1049,  858],\n",
    "        [1034,  278],\n",
    "        [1098,  469],\n",
    "        [1069,  626],\n",
    "        [1034,  482],\n",
    "        [1071,  398],\n",
    "        [1063,  858],\n",
    "        [1083,  443],\n",
    "        [1034,  418],\n",
    "        [1072,  632],\n",
    "        [1075,  914],\n",
    "        [1098, 1010],\n",
    "        [1094,  357],\n",
    "        [1087,  898],\n",
    "        [1084,  702],\n",
    "        [1099,  654],\n",
    "        [ 835,  364],\n",
    "        [ 208,  416],\n",
    "        [ 987,  722],\n",
    "        [ 872,  708],\n",
    "        [ 994,  399],\n",
    "        [ 264,  648],\n",
    "        [ 264, 1007],\n",
    "        [1001,  961],\n",
    "        [ 598,  320],\n",
    "        [ 360,  993],\n",
    "        [ 879,  747],\n",
    "        [ 325,  700],\n",
    "        [  52,  770],\n",
    "        [ 257,  268],\n",
    "        [ 257,  824],\n",
    "        [ 819,  662],\n",
    "        [ 709,  567],\n",
    "        [ 656,  662],\n",
    "        [  43,  602],\n",
    "        [1038,  742],\n",
    "        [  24,  964],\n",
    "        [1098,  289],\n",
    "        [1099,  722],\n",
    "        [ 855,  870],\n",
    "        [  25,  561],\n",
    "        [ 472,  519],\n",
    "        [ 472,  754],\n",
    "        [ 475, 1038],\n",
    "        [ 404,  857],\n",
    "        [ 331,  913],\n",
    "        [ 574,  434],\n",
    "        [ 537,  154],\n",
    "        [1022,  612],\n",
    "        [ 324,  321],\n",
    "        [ 937,  563],\n",
    "        [ 230, 1001],\n",
    "        [ 912,  563],\n",
    "        [ 912,  807],\n",
    "        [ 928,   99],\n",
    "        [ 928,   99],\n",
    "        [ 942,  228],\n",
    "        [ 604,  772],\n",
    "        [ 904,   94],\n",
    "        [ 472, 1063],\n",
    "        [  52,  812],\n",
    "        [  52,  645],\n",
    "        [  52,  697],\n",
    "        [ 257,  387],\n",
    "        [  52,  362],\n",
    "        [ 935,  247],\n",
    "        [ 983,   65],\n",
    "        [ 683,  874],\n",
    "        [ 155,  518],\n",
    "        [  30,  822],\n",
    "        [ 855,  467],\n",
    "        [ 904,  909],\n",
    "        [ 904,  529],\n",
    "        [ 904,  852],\n",
    "        [ 855,  399],\n",
    "        [ 855,  470],\n",
    "        [ 855, 1023],\n",
    "        [ 106,  870],\n",
    "        [ 176,  580],\n",
    "        [ 574,  669],\n",
    "        [ 502,  888],\n",
    "        [ 588,  708],\n",
    "        [ 782,  700],\n",
    "        [ 588,  743],\n",
    "        [ 890,  417],\n",
    "        [ 373,  822],\n",
    "        [ 160,  514],\n",
    "        [  47,  455],\n",
    "        [  47,  328],\n",
    "        [  47,  259],\n",
    "        [ 909,  971],\n",
    "        [1023,  962],\n",
    "        [ 577,  367]]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor = torch.clamp(audio_tensor, min=0, max=1023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([106, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_to_file(audio_tensor, \"general_out.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LJSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDWIDTH_IDX = 1 # original VALL-E\n",
    "CODEBOOKS     = [2, 4, 8, 16, 32]\n",
    "BANDWIDTHS    = [1.5, 3.0, 6.0, 12.0, 24.0]\n",
    "BANDWIDTH     = BANDWIDTHS[BANDWIDTH_IDX]\n",
    "CODEBOOK      = CODEBOOKS[BANDWIDTH_IDX]\n",
    "\n",
    "import torchaudio\n",
    "from ljspeech import LJSPEECH\n",
    "DATASET_PATH = \"./data/LJSpeech/\"\n",
    "dataset = LJSPEECH(\n",
    "    \"./data/LJSpeech\",\n",
    "    encodec_bandwidth=BANDWIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1919"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 143])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfileid_audio,\\nwaveform,\\nsample_rate,\\ntranscript,\\nnormalized_transcript,\\nphones,\\nphone_ids,\\ncodes\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.cuda.current_device()\n",
    "\n",
    "\"\"\"\n",
    "fileid_audio,\n",
    "waveform,\n",
    "sample_rate,\n",
    "transcript,\n",
    "normalized_transcript,\n",
    "phones,\n",
    "phone_ids,\n",
    "codes\n",
    "\"\"\"\n",
    "\n",
    "# def collate_fn(batch) -> torch.tensor:\n",
    "#     audio_tokens = []\n",
    "#     phone_tokens = []\n",
    "\n",
    "#     for item in batch:\n",
    "#         cur_aud_tok  = torch.tensor(item[7], device=device)\n",
    "#         cur_phonemes = torch.tensor(item[6], device=device)\n",
    "#         audio_tokens.append(cur_aud_tok)\n",
    "#         phone_tokens.append(cur_phonemes)\n",
    "\n",
    "#     # audio_tokens = torch.tensor(phone_tokens, device=device)\n",
    "#     audio_tokens = nn.utils.rnn.pad_sequence(audio_tokens, batch_first=True)\n",
    "#     # phone_tokens = torch.tensor(phone_tokens, device=device)\n",
    "#     phone_tokens = nn.utils.rnn.pad_sequence(phone_tokens, batch_first=True)\n",
    "\n",
    "#     return batch, phone_tokens, audio_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=1, sampler=train_sampler, collate_fn=lambda x: x)\n",
    "test_loader = DataLoader(dataset, batch_size=1, sampler=test_sampler, collate_fn=lambda x: x)\n",
    "\n",
    "# train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler, collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(dataset, batch_size=32, sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = next(iter(train_loader))\n",
    "# it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1727, 192)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(WindowsPath('data/LJSpeech/LJSpeech-1.1/wavs/LJ050-0137.wav'),\n",
       "  tensor([[ 0.0003,  0.0004,  0.0005,  ..., -0.0008, -0.0008, -0.0007]]),\n",
       "  22050,\n",
       "  'FBI, and the Secret Service.',\n",
       "  'FBI, and the Secret Service.',\n",
       "  ['B',\n",
       "   'AY1',\n",
       "   '_',\n",
       "   '_',\n",
       "   '_',\n",
       "   'AH0',\n",
       "   'N',\n",
       "   'D',\n",
       "   '_',\n",
       "   'DH',\n",
       "   'AH0',\n",
       "   '_',\n",
       "   'S',\n",
       "   'IY1',\n",
       "   'K',\n",
       "   'R',\n",
       "   'AH0',\n",
       "   'T',\n",
       "   '_',\n",
       "   'S',\n",
       "   'ER1',\n",
       "   'V',\n",
       "   'AH0',\n",
       "   'S',\n",
       "   '_',\n",
       "   '_'],\n",
       "  tensor([22, 20, 74, 74, 74, 10, 48, 24, 74, 25, 10, 74, 58, 42, 45, 57, 10, 60,\n",
       "          74, 58, 30, 69, 10, 58, 74, 74], device='cuda:0'),\n",
       "  tensor([[[ 865,   59,  309,  392,  695,  361,  706,  913,  822,  325,  176,\n",
       "             438,  438,  360,  360,  176,  176,  106,  257,  106,  106,  408,\n",
       "              63,  913,  801,  908,  801,  611,  530,  151,  944,  971,  347,\n",
       "             523,  855,   25,  593,  695,  723,  683,  169,  203,  760,  683,\n",
       "             240,  925,  925,   20,  162,  216,  216,  216,  793,  793,  901,\n",
       "             402,  216,  216,  291,  495,  881,  495,  598,  860,  136,  699,\n",
       "             430,  855,  835,  876,  738,  408,  106,  738,  106,  738,  106,\n",
       "             738,  738,  738,  738,  738,  738,  738,  106,  738,  408,  408,\n",
       "             408,  408,  408,  408,  408,  408,  408,  408,  408,  408,  408,\n",
       "             408,  408,  408,  408,  677,  804, 1006,  588,  659,  788,  222,\n",
       "             645, 1021,  645, 1022,  208,  860,  598, 1001,  208,  325,  934,\n",
       "             890,  784,  944,  148,  148,  574,  574,   47,  574,  160,  574,\n",
       "             574,  574,  574,   53,  433,  945,  530,  611,  344,  208,  339,\n",
       "             106, 1017,  430, 1017,   91,  475,  323,  936,  987,   23,  151,\n",
       "             148,  106, 1019,  160,  574,   47,   47,  160,   47,  574,  574,\n",
       "             463,   25,  324,  392,  476,  658,  694,  658,  983,  185,  185,\n",
       "             890,  879,  879,  395,  325,  185,  185,  523,  983,  208,   30,\n",
       "             779,  432,  276,  463,  148,  148,  160,  160,  160,  463,  373,\n",
       "             160,  160,  160,  148,  463,   25,  738,  106,  835,  855,  408,\n",
       "             835,  738,  738,  408,  738],\n",
       "           [ 687,  570,  869,  271,   98,  559, 1011,  696,  222,  516,  959,\n",
       "             948,  836,  785, 1023,  928,  993,  993,  913,  928,  928,  928,\n",
       "             182,  844,  662,  662,  846,  846,  378,  414,  336,  559,  964,\n",
       "            1016,  800,  420,  222,   27,  549,  549,  555,  549,    4,  549,\n",
       "             549, 1001, 1001,  390,  905,  596,  295,  685,  435,  834,  527,\n",
       "             592,   14,  446,  704,  602,  602,  357,  966,  824,  259,  880,\n",
       "             877,  870,  700,  404,  700,  424,  424,  518,  913,  518,  913,\n",
       "             913,  544,  424,  544,  544,  544,  544,  913,  518,  424,  424,\n",
       "             424,  424,  424,  424,  424,  424,  518,  518,  424,  913,  913,\n",
       "             518,  424,  518,  518,  791,  833,  565,  177,   75,  400,  444,\n",
       "             668,  598,  745,  289,  345,  414,  948,  259,  841,  896,  748,\n",
       "             252,  307,  564,  211,  452,   71,    4,  973,  646,  910,  160,\n",
       "            1010,  758,  754,  185,  727,  140,  517,   16,   74,  857,  516,\n",
       "             857,  857,  758,  758,  984,  961,  721,  351,  182,  265,  673,\n",
       "             336,  964,  424,  973, 1010,  541,  541,  857,  973,  910,  993,\n",
       "             974,  222,  615,  243,  527,  441,  307,  269,  349,  444,  945,\n",
       "             269,  496,  236,  363,  458,  534,  880,    0,  458,  700,  560,\n",
       "             388,  964,  580,  909,  541,  857,   36,  541,  160, 1010, 1010,\n",
       "             209,  209,  471,  133,   92,  957,  518,  913,  518,  857,  424,\n",
       "             424,  913,  913,  424,  544],\n",
       "           [ 970,   52,  538,  538,  659,  344,  287,  476,  225,  698,  843,\n",
       "             711,  361,  814,  188,  893,  551,  893,  893,  893,  893,  989,\n",
       "             868,  951,  381,  435,   47,   47,  365,  918,  510,  255,  618,\n",
       "             711,  852,  767,  116,   94,  500,  979,  500,  302,  694,  707,\n",
       "             132,  641,  641,  864,  413,  864,  542,  296,  405,  560,   64,\n",
       "             672,  467,  864,  823,  911,  906,  868,  431, 1012,  551,  918,\n",
       "             198,  428, 1000,  675,  653,  982,  678,  982,  937,   36,  937,\n",
       "             937,  786,  678,  786, 1007,  653,  653,  937,  982,  786,  786,\n",
       "              36,   36,  786,  653,  653,   36,   36,   36,   36,   36,  937,\n",
       "             786,   36,  786,   36,  711,  255,  627,  413,  737,  235,  532,\n",
       "             572,  532,  606,   23,  110,  875,  599,  928,  880,  675,  798,\n",
       "             451,  997,  883,  907,  907,  242,  706,  814,  451,  326,  451,\n",
       "             432,  590,  432,  451,  911,  803,  451,  451,  565,  933,  508,\n",
       "             819,  982,  915,  982,  618,  705,  898,  705,  510,  316,  832,\n",
       "             486,  934,  653,  601,  432,  702,  864,  451,  228,  601,  590,\n",
       "             227,   93,  324,  416,  940,  386,  180,  644,  688,  798,  908,\n",
       "             450,  856,  225,  982,  626,  493,  335,  758,  316,  759,  675,\n",
       "             406,  711,  769,  918,  977,  526,  267,  432,  432, 1005,  936,\n",
       "             813,  541,  432,  936,  937,  907,  730, 1005, 1005,   36,  653,\n",
       "             982,  653,  786,   36,   36],\n",
       "           [ 866,  864,  397,  357,  402,  424,  723,  612,  577,  863,  261,\n",
       "             940,  440,  638,  940,  453,  443,  255, 1019,  919,  673,  215,\n",
       "             963,  928,  453,  601,  981,  547,   12,  962,  444,  797,  733,\n",
       "             960,  529,  597,  612,  118,  562,  870,  110,  402,  104,  103,\n",
       "              50,  157,  608,  697,  791,  249,  885,  204,  457,  104,  137,\n",
       "             200,  755,  233,  627,  882,  855,  823,  939,  659,  326,  384,\n",
       "             222,   74,  762,  793,  793,  741,  673,  673,  673,  866,  673,\n",
       "             741,  673,  859,  673,  673,  673,  673,  673,  741,  741,  741,\n",
       "             741,  741,  741,  741,  741,  741,  741,  741,  741,  741,  741,\n",
       "             741,  741,  741,  741,  721,  410,   50,  259,   16,  282,  791,\n",
       "             282,  885,  885,  315,  875,  675,  524,  714,  962,  255,  110,\n",
       "             295,  638,  515,  990,  762,  651,  443,  651,  376,  180,  940,\n",
       "             601,  601,  440,  940,  326,  440,  107,  644,  440,  440,  255,\n",
       "             962,  580,  859,  838,  440,  780,  766,   49,  461,  448,  721,\n",
       "            1001,  558,  443,  519,  721,  834,  440,  612,  440,  440,  200,\n",
       "             255, 1022,  960,  787,  851,  190,  712,  692,  818,  522, 1007,\n",
       "             757,  224,  686,  940,  734,  517,  125,  924,  589,  140,   49,\n",
       "             274,  962,  962,  242,  443,  961,  854,  398,  418,  778,  778,\n",
       "              75,  443,  651,  434,  916, 1016,  318,  366,  651,  762,  741,\n",
       "             673,  866,  866,  741,  673]]], device='cuda:0'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import megabyte\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "def get_reserved_mem_gb():\n",
    "    device = torch.cuda.current_device()\n",
    "    reserved = torch.cuda.memory_reserved(device)\n",
    "    reserved_gb = reserved / 1024 / 1024 / 1024\n",
    "    return reserved_gb\n",
    "\n",
    "class PhoneLM(nn.Module):\n",
    "    def __init__(self, n_phone_tokens, n_audio_tokens):\n",
    "        super(PhoneLM, self).__init__()\n",
    "        self.megabyte   = megabyte.MEGABYTE(\n",
    "            heads       = 8, # 1,\n",
    "            dim_head    = 32, # 16,\n",
    "            num_tokens  = n_phone_tokens + n_audio_tokens + 4,\n",
    "            dim         = (768, 256, 128), # (32, 32, 32), # (768, 256, 128)# Dg, Dl1, Dl2\n",
    "            depth       = (6, 4, 2), # (6, 4, 2)\n",
    "            max_seq_len = (32, 4, 4), # (128, 4, 4), # , # 512\n",
    "            flash_attn  = False)\n",
    "\n",
    "    def forward(self, x, debug=False, return_loss=True):\n",
    "        x = self.megabyte(x, return_loss=return_loss)\n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        o = [param.numel() for param in self.parameters() if param.requires_grad]\n",
    "        o = sum(o)\n",
    "        return o\n",
    "    \n",
    "    def generate(self, *args):\n",
    "        return self.megabyte.generate(*args)\n",
    "    \n",
    "def multi_encode(\n",
    "        phone_tokens,\n",
    "        audio_tokens,\n",
    "        n_phone_tokens,\n",
    "        n_audio_tokens,\n",
    "        max_clip_length=1.0):\n",
    "    \"\"\"NOTE: 75 steps per second for 24kHz in `encodec.\n",
    "    Set `max_clip_length` to 0 for original clip length.\"\"\"\n",
    "\n",
    "    # Start text token, end text token, start audio token, end audio token\n",
    "    ETT, EAT = [n_phone_tokens + n_audio_tokens + i\n",
    "                          for i in range(2)]\n",
    "    ETT = torch.tensor([ETT]).long().cuda()\n",
    "    EAT = torch.tensor([EAT]).long().cuda()\n",
    "\n",
    "    if max_clip_length:\n",
    "        #print(\"pre audio_tokens.shape\", audio_tokens.shape)\n",
    "        audio_tokens = audio_tokens[:, :, :int(max_clip_length * 75)]\n",
    "    audio_tokens = rearrange(audio_tokens.squeeze(0), \"q s -> (q s)\")\n",
    "    #print(\"post audio_tokens.shape\", audio_tokens.shape)\n",
    "    \n",
    "    # offset phone tokens past audio tokens\n",
    "    phone_tokens += n_audio_tokens\n",
    "    \n",
    "    #print(\"phone_tokens.shape:\", phone_tokens.shape)\n",
    "    #print(\"audio_tokens.shape:\", audio_tokens.shape)\n",
    "    \n",
    "    device = torch.cuda.current_device()\n",
    "    phone_tokens = torch.cat((phone_tokens, ETT), dim=0).to(device)\n",
    "    audio_tokens = torch.cat((audio_tokens, EAT,), dim=0).to(device)\n",
    "    combined_tokens = torch.cat((phone_tokens, audio_tokens), dim=0).to(device)\n",
    "    return phone_tokens, audio_tokens, combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "from encodec_util import decode_to_file\n",
    "\n",
    "\"\"\"\n",
    "EinopsError:  Error while processing rearrange-reduction pattern \"(t q) -> t q\".\n",
    " Input tensor shape: torch.Size([75]). Additional info: {'q': 4, 't': 75}.\n",
    " Shape mismatch, 75 != 300\n",
    "\"\"\"\n",
    "\n",
    "def generate_audio(sample,\n",
    "                   n_phone_tokens,\n",
    "                   n_audio_tokens,\n",
    "                   audio_path=\"./out.wav\"):\n",
    "    ETT, EAT = [n_phone_tokens + n_audio_tokens + i\n",
    "                          for i in range(2)]\n",
    "    ST_S = [ETT, EAT]\n",
    "    print(\"ETT, EAT ids:\", ST_S)\n",
    "    seq = sample.cpu().tolist()[0]\n",
    "    print(\"seq:\", seq)\n",
    "    # all special tokens in list\n",
    "    if all(st_t in seq for st_t in ST_S) and len(seq) >= len(ST_S) + 2:\n",
    "        # text_tokens  = seq[seq.index(STT + 1):seq.index(ETT - 1)]\n",
    "        audio_tokens = seq[seq.index(ETT)+1:seq.index(EAT)]\n",
    "        print(seq.index(ETT), seq.index(EAT), len(audio_tokens))\n",
    "        audio_tokens = torch.tensor(audio_tokens).cuda()\n",
    "        audio_tokens = rearrange(\n",
    "            audio_tokens,\n",
    "            '(t q) -> t q',\n",
    "            q=1, # CODEBOOK,\n",
    "            t=audio_tokens.size(0) // 1) # t=audio_tokens.size(0) // CODEBOOK)\n",
    "        print(\"audio_tokens.shape:\", audio_tokens, audio_tokens.shape)\n",
    "        decode_to_file(audio_tokens, audio_path)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhoneLM - LJSpeech (Overfit Multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 37.30M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37302863"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PhoneLM(\n",
    "    n_phone_tokens=len(dataset.phone_dict),\n",
    "    n_audio_tokens=1024).to(device)\n",
    "\n",
    "model.megabyte.get_num_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(train_loader))\n",
    "# item_phone_tokens = item[-2]\n",
    "# # item_audio_tokens = item[-1]\n",
    "# item_audio_tokens = item[-1][:, 0, :] # Only keep primary coarse tokens, for now\n",
    "# item_audio_tokens = item_audio_tokens.unsqueeze(0)\n",
    "# item_phone_tokens.shape, item_audio_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to taking the descriptions of newly-arrived prisoners.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phone_prompt, audio_target, test_inp = multi_encode(\n",
    "#     item_phone_tokens,\n",
    "#     item_audio_tokens,\n",
    "#     n_phone_tokens=len(dataset.phone_dict),\n",
    "#     n_audio_tokens=1024,\n",
    "#     max_clip_length=5)\n",
    "# test_inp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "MAX_LR       = 1e-2\n",
    "# MAX_LR       = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GRAD_CLIP    = 0.1\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=MAX_LR)\n",
    "    #,weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# def get_lr(optimizer):\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         return param_group['lr']\n",
    "\n",
    "# sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, MAX_LR, epochs=epochs, \n",
    "#                                                 steps_per_epoch=len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20876/3201252558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20876/3201252558.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, trainloader)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# print(\"batch:\", batch.shape, batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20876/3374421788.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, debug, return_loss)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmegabyte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\OneDrive\\Desktop\\projects\\PhoneLM\\megabyte.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ids, return_loss)\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[0mstage_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage_tokens\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprev_stage_tokens_repr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m             \u001b[0mattended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[0mattended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpack_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattended\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'* n d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\OneDrive\\Desktop\\projects\\PhoneLM\\megabyte.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mff\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_shift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotary_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrotary_emb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_shift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\OneDrive\\Desktop\\projects\\PhoneLM\\megabyte.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, rotary_emb)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_kv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b n (h d) -> b h n d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\OneDrive\\Desktop\\projects\\PhoneLM\\megabyte.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\win8t\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1499\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"fro\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1501\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "EPOCHS = 1000\n",
    "PRINT_INTERVAL = 100\n",
    "\n",
    "seq_len = 512 # 2048\n",
    "\n",
    "# phone_prompt, audio_target, test_inp = multi_encode(\n",
    "#     item[0][6], # phone tokens\n",
    "#     item[0][7], # audio tokens\n",
    "#     n_phone_tokens=len(dataset.phone_dict),\n",
    "#     n_audio_tokens=1024,\n",
    "#     max_clip_length=1)\n",
    "\n",
    "# model = PhoneLM(\n",
    "#     n_phone_tokens=len(dataset.phone_dict),\n",
    "#     n_audio_tokens=1024).to(device)\n",
    "\n",
    "prompt = None\n",
    "\n",
    "def create_seq(item_phone_tokens, item_audio_tokens):\n",
    "    global prompt\n",
    "    phone_prompt, audio_target, test_inp = multi_encode(\n",
    "        item_phone_tokens,\n",
    "        item_audio_tokens,\n",
    "        n_phone_tokens=len(dataset.phone_dict),\n",
    "        n_audio_tokens=1024,\n",
    "        max_clip_length=1)\n",
    "    prompt = phone_prompt\n",
    "    padding_len = max(0, seq_len - test_inp.size(0))\n",
    "    n_test_inp  = F.pad(test_inp, (0, padding_len))\n",
    "    cur_item    = n_test_inp\n",
    "    # cur_item = n_test_inp.unsqueeze(0)\n",
    "    return cur_item\n",
    "\n",
    "def create_batch(batch):\n",
    "    rnn_batch = []\n",
    "    for item in batch:\n",
    "        item_phone_tokens = item[6]\n",
    "        item_audio_tokens = item[7]\n",
    "        seq = create_seq(item_phone_tokens, item_audio_tokens)\n",
    "        rnn_batch.append(seq)\n",
    "    rnn_batch = nn.utils.rnn.pad_sequence(rnn_batch, batch_first=True)\n",
    "    return rnn_batch\n",
    "\n",
    "cur_batch = item\n",
    "batch = create_batch(cur_batch)\n",
    "\n",
    "def train(model, trainloader):\n",
    "    model.train()\n",
    "\n",
    "    # print(\"batch:\", batch.shape, batch)\n",
    "    loss = model(batch, return_loss=True)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# def train(model, trainloader):\n",
    "#     model.train()\n",
    "    \n",
    "#     padding_len = max(0, seq_len - test_inp.size(0))\n",
    "#     n_test_inp = F.pad(test_inp, (0, padding_len))\n",
    "#     batch = n_test_inp.unsqueeze(0)\n",
    "#     print(\"batch.shape:\", batch.shape, batch)\n",
    "#     loss = model(batch, return_loss=True)\n",
    "#     # loss = model(next(trainloader), return_loss=True)\n",
    "#     loss.backward()\n",
    "#     return loss\n",
    "\n",
    "# pbar = tqdm.tqdm(EPOCHS, mininterval=10., desc='training')\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    loss = train(model, train_loader)\n",
    "    optimizer.step()\n",
    "    \n",
    "    mem_gb = get_reserved_mem_gb()\n",
    "    if epoch % PRINT_INTERVAL == 0:\n",
    "        print(f\"Reserved Memory (GB): {mem_gb}, loss: {loss.item()}\")\n",
    "    # pbar.set_description(f\"Reserved Memory (GB): {mem_gb}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1059, 1081, 1057, 1097, 1053, 1098, 1084, 1075, 1070, 1048, 1098, 1049,\n",
       "         1034, 1098, 1069, 1034, 1071, 1063, 1083, 1034, 1072, 1098, 1098, 1098,\n",
       "         1069, 1094, 1075, 1084, 1098, 1098, 1099]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUCKING HELLO?\n",
      "prompt: torch.Size([1, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 481/481 [00:06<00:00, 74.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: tensor([[1059, 1081, 1057, 1097, 1053, 1098, 1084, 1075, 1070, 1048, 1098, 1049,\n",
      "         1034, 1098, 1069, 1034, 1071, 1063, 1083, 1034, 1072, 1098, 1098, 1098,\n",
      "         1069, 1094, 1075, 1084, 1098, 1098, 1099,  835,  160,  438,  488,  887,\n",
      "          203,  503,  441,    6,   81,  727,  141,  908,  908,  502,  303,  148,\n",
      "          103,  496,  145,  731,  977,  259,  582,  808,  921,  432,  779,  779,\n",
      "          472,  472,  331,  103,  887,  457,  987,  501,  921,  197,  197,  931,\n",
      "          928,  881,  834,  432,  604,  491,  373,  994,  782,  834,  408,  855,\n",
      "          855,  798,  176,  798,  537,  779,  936,  457,  751,  651,  687,  751,\n",
      "          790,  686,  994,   57,  916,  751,  699,  145,  145,  148, 1010,    4,\n",
      "          973,  984, 1002,    3,  472,  185,  662,  662,  471,  471,  106,  734,\n",
      "          893,  802,  285, 1010,  812,  272,  529,  930,  486,  323,  632,  466,\n",
      "          930,  601,  646,  924,  913,  160, 1010,  857,  984,  668,  399,  399,\n",
      "          399,  710,   27,  710,  710,  564,  765,  888,  770,  404,  405,  420,\n",
      "          708,  752,  928,   71,  216,   43,  471,  801,  404,  112,  214,   48,\n",
      "          425, 1011,   48,  767,  792,  792,  869,  268,  471,  200, 1010,  857,\n",
      "          857,  970,  457,  212,  188,  212,   54,   90,  573,  999,  298,  977,\n",
      "          469,  977,  977,  587,  977,  590,  747,  590,  590,  906,  775,  536,\n",
      "          624,  264,  432,  705,  647,  832, 1007,  861,  758,  633, 1015, 1007,\n",
      "          759,   34,  733,  733,  741,  659,  757,  932,  626,  829,  832,  547,\n",
      "          752,  701,  955,  242,  705, 1000,  710,  907,   80,  893,  438,  893,\n",
      "          932,  549,   55,  442,  426,  831,  581,  475,  977,   93,   93,  712,\n",
      "          298,  432,  933,  242,  268,  919,  261,  787,  255,  263,  787,  787,\n",
      "          791,   19,  238,  490,  416,  935,  698,  588,  889,  411,  778,  779,\n",
      "          859,  817,  162,  418,  211,  306,  255,  714,  993,  875,  493,  418,\n",
      "          762, 1019,  178,  504,  414,  605,  499,  706,  628,  214,   78,  558,\n",
      "         1019, 1000, 1019,  874,  110,  804,  381,  529,  866,   36,  803,  940,\n",
      "          180,  941,  733,   96,  803,  335,  524,  714,  224,  733,  499,  769,\n",
      "          204,  273,  994,  249,  882,  940,  148, 1100,  993,  875,  493,  418,\n",
      "          762, 1019,  178,  504,  414,  605,  499,  706,  628,  214,   78,  558,\n",
      "         1019, 1000, 1019,  874,  110,  804,  381,  529,  866,   36,  803,  940,\n",
      "          180,  941,  733,   96,  803,  335,  524,  714,  224,  733,  499,  769,\n",
      "          204,  273,  994,  249,  882,  940,  148, 1100,  993,  875,  493,  418,\n",
      "          762, 1019,  178,  504,  414,  605,  499,  706,  628,  214,   78,  558,\n",
      "         1019, 1000, 1019,  874,  110,  804,  381,  529,  866,   36,  803,  940,\n",
      "          180,  941,  733,   96,  803,  335,  524,  714,  224,  733,  499,  769,\n",
      "          204,  273,  994,  249,  882,  940,  148, 1100,  573,  999,  298,  977,\n",
      "          469,  977,  977,  587,  977,  590,  747,  590,  590,  906,  775,  536,\n",
      "          624,  264,  432,  705,  647,  832, 1007,  861,  758,  633, 1015, 1007,\n",
      "          759,   34,  733,  733,  741,  659,  757,  932,  626,  829,  832,  547,\n",
      "          752,  701,  955,  242,  705, 1000,  710,  907,   80,  893,  438,  893,\n",
      "          932,  549,   55,  442,  426,  831,  581,  475,  977,   93,   93,  712,\n",
      "          298,  432,  933,  242,  268,  919,  261,  787,  255,  263,  787,  787,\n",
      "          791,   19,  238,  490,  416,  935,  698,  588]], device='cuda:0') torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate(model, inp):\n",
    "    model.eval()\n",
    "\n",
    "    # inp = inp.unsqueeze(0)\n",
    "    sample = model.generate(inp)\n",
    "    sample = sample.flatten(1)\n",
    "    print(\"sample:\", sample, sample.shape)\n",
    "\n",
    "    return prompt, sample\n",
    "\n",
    "prompt, sample = generate(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099 1100\n"
     ]
    }
   ],
   "source": [
    "ETT, EAT = [len(dataset.phone_dict) + 1024 + i\n",
    "                          for i in range(2)]\n",
    "print(ETT, EAT)\n",
    "# sample.index(STT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETT, EAT ids: [1099, 1100]\n",
      "seq: [1059, 1081, 1057, 1097, 1053, 1098, 1084, 1075, 1070, 1048, 1098, 1049, 1034, 1098, 1069, 1034, 1071, 1063, 1083, 1034, 1072, 1098, 1098, 1098, 1069, 1094, 1075, 1084, 1098, 1098, 1099, 835, 160, 438, 488, 887, 203, 503, 441, 6, 81, 727, 141, 908, 908, 502, 303, 148, 103, 496, 145, 731, 977, 259, 582, 808, 921, 432, 779, 779, 472, 472, 331, 103, 887, 457, 987, 501, 921, 197, 197, 931, 928, 881, 834, 432, 604, 491, 373, 994, 782, 834, 408, 855, 855, 798, 176, 798, 537, 779, 936, 457, 751, 651, 687, 751, 790, 686, 994, 57, 916, 751, 699, 145, 145, 148, 1010, 4, 973, 984, 1002, 3, 472, 185, 662, 662, 471, 471, 106, 734, 893, 802, 285, 1010, 812, 272, 529, 930, 486, 323, 632, 466, 930, 601, 646, 924, 913, 160, 1010, 857, 984, 668, 399, 399, 399, 710, 27, 710, 710, 564, 765, 888, 770, 404, 405, 420, 708, 752, 928, 71, 216, 43, 471, 801, 404, 112, 214, 48, 425, 1011, 48, 767, 792, 792, 869, 268, 471, 200, 1010, 857, 857, 970, 457, 212, 188, 212, 54, 90, 573, 999, 298, 977, 469, 977, 977, 587, 977, 590, 747, 590, 590, 906, 775, 536, 624, 264, 432, 705, 647, 832, 1007, 861, 758, 633, 1015, 1007, 759, 34, 733, 733, 741, 659, 757, 932, 626, 829, 832, 547, 752, 701, 955, 242, 705, 1000, 710, 907, 80, 893, 438, 893, 932, 549, 55, 442, 426, 831, 581, 475, 977, 93, 93, 712, 298, 432, 933, 242, 268, 919, 261, 787, 255, 263, 787, 787, 791, 19, 238, 490, 416, 935, 698, 588, 889, 411, 778, 779, 859, 817, 162, 418, 211, 306, 255, 714, 993, 875, 493, 418, 762, 1019, 178, 504, 414, 605, 499, 706, 628, 214, 78, 558, 1019, 1000, 1019, 874, 110, 804, 381, 529, 866, 36, 803, 940, 180, 941, 733, 96, 803, 335, 524, 714, 224, 733, 499, 769, 204, 273, 994, 249, 882, 940, 148, 1100, 993, 875, 493, 418, 762, 1019, 178, 504, 414, 605, 499, 706, 628, 214, 78, 558, 1019, 1000, 1019, 874, 110, 804, 381, 529, 866, 36, 803, 940, 180, 941, 733, 96, 803, 335, 524, 714, 224, 733, 499, 769, 204, 273, 994, 249, 882, 940, 148, 1100, 993, 875, 493, 418, 762, 1019, 178, 504, 414, 605, 499, 706, 628, 214, 78, 558, 1019, 1000, 1019, 874, 110, 804, 381, 529, 866, 36, 803, 940, 180, 941, 733, 96, 803, 335, 524, 714, 224, 733, 499, 769, 204, 273, 994, 249, 882, 940, 148, 1100, 573, 999, 298, 977, 469, 977, 977, 587, 977, 590, 747, 590, 590, 906, 775, 536, 624, 264, 432, 705, 647, 832, 1007, 861, 758, 633, 1015, 1007, 759, 34, 733, 733, 741, 659, 757, 932, 626, 829, 832, 547, 752, 701, 955, 242, 705, 1000, 710, 907, 80, 893, 438, 893, 932, 549, 55, 442, 426, 831, 581, 475, 977, 93, 93, 712, 298, 432, 933, 242, 268, 919, 261, 787, 255, 263, 787, 787, 791, 19, 238, 490, 416, 935, 698, 588]\n",
      "30 331 300\n",
      "audio_tokens.shape: tensor([[ 835],\n",
      "        [ 160],\n",
      "        [ 438],\n",
      "        [ 488],\n",
      "        [ 887],\n",
      "        [ 203],\n",
      "        [ 503],\n",
      "        [ 441],\n",
      "        [   6],\n",
      "        [  81],\n",
      "        [ 727],\n",
      "        [ 141],\n",
      "        [ 908],\n",
      "        [ 908],\n",
      "        [ 502],\n",
      "        [ 303],\n",
      "        [ 148],\n",
      "        [ 103],\n",
      "        [ 496],\n",
      "        [ 145],\n",
      "        [ 731],\n",
      "        [ 977],\n",
      "        [ 259],\n",
      "        [ 582],\n",
      "        [ 808],\n",
      "        [ 921],\n",
      "        [ 432],\n",
      "        [ 779],\n",
      "        [ 779],\n",
      "        [ 472],\n",
      "        [ 472],\n",
      "        [ 331],\n",
      "        [ 103],\n",
      "        [ 887],\n",
      "        [ 457],\n",
      "        [ 987],\n",
      "        [ 501],\n",
      "        [ 921],\n",
      "        [ 197],\n",
      "        [ 197],\n",
      "        [ 931],\n",
      "        [ 928],\n",
      "        [ 881],\n",
      "        [ 834],\n",
      "        [ 432],\n",
      "        [ 604],\n",
      "        [ 491],\n",
      "        [ 373],\n",
      "        [ 994],\n",
      "        [ 782],\n",
      "        [ 834],\n",
      "        [ 408],\n",
      "        [ 855],\n",
      "        [ 855],\n",
      "        [ 798],\n",
      "        [ 176],\n",
      "        [ 798],\n",
      "        [ 537],\n",
      "        [ 779],\n",
      "        [ 936],\n",
      "        [ 457],\n",
      "        [ 751],\n",
      "        [ 651],\n",
      "        [ 687],\n",
      "        [ 751],\n",
      "        [ 790],\n",
      "        [ 686],\n",
      "        [ 994],\n",
      "        [  57],\n",
      "        [ 916],\n",
      "        [ 751],\n",
      "        [ 699],\n",
      "        [ 145],\n",
      "        [ 145],\n",
      "        [ 148],\n",
      "        [1010],\n",
      "        [   4],\n",
      "        [ 973],\n",
      "        [ 984],\n",
      "        [1002],\n",
      "        [   3],\n",
      "        [ 472],\n",
      "        [ 185],\n",
      "        [ 662],\n",
      "        [ 662],\n",
      "        [ 471],\n",
      "        [ 471],\n",
      "        [ 106],\n",
      "        [ 734],\n",
      "        [ 893],\n",
      "        [ 802],\n",
      "        [ 285],\n",
      "        [1010],\n",
      "        [ 812],\n",
      "        [ 272],\n",
      "        [ 529],\n",
      "        [ 930],\n",
      "        [ 486],\n",
      "        [ 323],\n",
      "        [ 632],\n",
      "        [ 466],\n",
      "        [ 930],\n",
      "        [ 601],\n",
      "        [ 646],\n",
      "        [ 924],\n",
      "        [ 913],\n",
      "        [ 160],\n",
      "        [1010],\n",
      "        [ 857],\n",
      "        [ 984],\n",
      "        [ 668],\n",
      "        [ 399],\n",
      "        [ 399],\n",
      "        [ 399],\n",
      "        [ 710],\n",
      "        [  27],\n",
      "        [ 710],\n",
      "        [ 710],\n",
      "        [ 564],\n",
      "        [ 765],\n",
      "        [ 888],\n",
      "        [ 770],\n",
      "        [ 404],\n",
      "        [ 405],\n",
      "        [ 420],\n",
      "        [ 708],\n",
      "        [ 752],\n",
      "        [ 928],\n",
      "        [  71],\n",
      "        [ 216],\n",
      "        [  43],\n",
      "        [ 471],\n",
      "        [ 801],\n",
      "        [ 404],\n",
      "        [ 112],\n",
      "        [ 214],\n",
      "        [  48],\n",
      "        [ 425],\n",
      "        [1011],\n",
      "        [  48],\n",
      "        [ 767],\n",
      "        [ 792],\n",
      "        [ 792],\n",
      "        [ 869],\n",
      "        [ 268],\n",
      "        [ 471],\n",
      "        [ 200],\n",
      "        [1010],\n",
      "        [ 857],\n",
      "        [ 857],\n",
      "        [ 970],\n",
      "        [ 457],\n",
      "        [ 212],\n",
      "        [ 188],\n",
      "        [ 212],\n",
      "        [  54],\n",
      "        [  90],\n",
      "        [ 573],\n",
      "        [ 999],\n",
      "        [ 298],\n",
      "        [ 977],\n",
      "        [ 469],\n",
      "        [ 977],\n",
      "        [ 977],\n",
      "        [ 587],\n",
      "        [ 977],\n",
      "        [ 590],\n",
      "        [ 747],\n",
      "        [ 590],\n",
      "        [ 590],\n",
      "        [ 906],\n",
      "        [ 775],\n",
      "        [ 536],\n",
      "        [ 624],\n",
      "        [ 264],\n",
      "        [ 432],\n",
      "        [ 705],\n",
      "        [ 647],\n",
      "        [ 832],\n",
      "        [1007],\n",
      "        [ 861],\n",
      "        [ 758],\n",
      "        [ 633],\n",
      "        [1015],\n",
      "        [1007],\n",
      "        [ 759],\n",
      "        [  34],\n",
      "        [ 733],\n",
      "        [ 733],\n",
      "        [ 741],\n",
      "        [ 659],\n",
      "        [ 757],\n",
      "        [ 932],\n",
      "        [ 626],\n",
      "        [ 829],\n",
      "        [ 832],\n",
      "        [ 547],\n",
      "        [ 752],\n",
      "        [ 701],\n",
      "        [ 955],\n",
      "        [ 242],\n",
      "        [ 705],\n",
      "        [1000],\n",
      "        [ 710],\n",
      "        [ 907],\n",
      "        [  80],\n",
      "        [ 893],\n",
      "        [ 438],\n",
      "        [ 893],\n",
      "        [ 932],\n",
      "        [ 549],\n",
      "        [  55],\n",
      "        [ 442],\n",
      "        [ 426],\n",
      "        [ 831],\n",
      "        [ 581],\n",
      "        [ 475],\n",
      "        [ 977],\n",
      "        [  93],\n",
      "        [  93],\n",
      "        [ 712],\n",
      "        [ 298],\n",
      "        [ 432],\n",
      "        [ 933],\n",
      "        [ 242],\n",
      "        [ 268],\n",
      "        [ 919],\n",
      "        [ 261],\n",
      "        [ 787],\n",
      "        [ 255],\n",
      "        [ 263],\n",
      "        [ 787],\n",
      "        [ 787],\n",
      "        [ 791],\n",
      "        [  19],\n",
      "        [ 238],\n",
      "        [ 490],\n",
      "        [ 416],\n",
      "        [ 935],\n",
      "        [ 698],\n",
      "        [ 588],\n",
      "        [ 889],\n",
      "        [ 411],\n",
      "        [ 778],\n",
      "        [ 779],\n",
      "        [ 859],\n",
      "        [ 817],\n",
      "        [ 162],\n",
      "        [ 418],\n",
      "        [ 211],\n",
      "        [ 306],\n",
      "        [ 255],\n",
      "        [ 714],\n",
      "        [ 993],\n",
      "        [ 875],\n",
      "        [ 493],\n",
      "        [ 418],\n",
      "        [ 762],\n",
      "        [1019],\n",
      "        [ 178],\n",
      "        [ 504],\n",
      "        [ 414],\n",
      "        [ 605],\n",
      "        [ 499],\n",
      "        [ 706],\n",
      "        [ 628],\n",
      "        [ 214],\n",
      "        [  78],\n",
      "        [ 558],\n",
      "        [1019],\n",
      "        [1000],\n",
      "        [1019],\n",
      "        [ 874],\n",
      "        [ 110],\n",
      "        [ 804],\n",
      "        [ 381],\n",
      "        [ 529],\n",
      "        [ 866],\n",
      "        [  36],\n",
      "        [ 803],\n",
      "        [ 940],\n",
      "        [ 180],\n",
      "        [ 941],\n",
      "        [ 733],\n",
      "        [  96],\n",
      "        [ 803],\n",
      "        [ 335],\n",
      "        [ 524],\n",
      "        [ 714],\n",
      "        [ 224],\n",
      "        [ 733],\n",
      "        [ 499],\n",
      "        [ 769],\n",
      "        [ 204],\n",
      "        [ 273],\n",
      "        [ 994],\n",
      "        [ 249],\n",
      "        [ 882],\n",
      "        [ 940],\n",
      "        [ 148]], device='cuda:0') torch.Size([300, 1])\n"
     ]
    }
   ],
   "source": [
    "out = generate_audio(\n",
    "    sample,\n",
    "    n_phone_tokens=len(dataset.phone_dict),\n",
    "    n_audio_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhoneLM - LJSpeech (Generalise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
